\chapter{Arquitectura de los modelos de Inteligencia Artificial entrenados}
\label{appendix-b}
\section{VAE con espectrogramas MEL}

\begin{table}[H]
\centering
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
=== Encoder ===
VAE_Encoder(
  (encoder): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
  )
  (genre_embedding): Embedding(5, 8)
  (conv_mu): Conv2d(264, 32, kernel_size=(1, 1), stride=(1, 1))
  (conv_logvar): Conv2d(264, 32, kernel_size=(1, 1), stride=(1, 1))
)

=== Decoder ===
VAE_Decoder(
  (genre_embedding): Embedding(5, 8)
  (lstm): LSTM(1032, 2048, num_layers=2, batch_first=True, dropout=0.3)
  (fc_out): Linear(in_features=2048, out_features=1024, bias=True)
  (deconv): Sequential(
    (0): ConvTranspose2d(32, 256, kernel_size=(4, 1), stride=(2, 1), padding=(1, 0))
    (1): LeakyReLU(negative_slope=0.2)
    (2): ConvTranspose2d(256, 128, kernel_size=(4, 1), stride=(2, 1), padding=(1, 0))
    (3): LeakyReLU(negative_slope=0.2)
    (4): ConvTranspose2d(128, 1, kernel_size=(4, 1), stride=(2, 1), padding=(1, 0))
  )
)

\end{lstlisting}
\end{minipage}
\caption{Resumen de arquitectura del modelo VAE - MEL (encoder y decoder).}
\label{tab:vae_mel_model_summary}
\end{table}

\begin{table}
\centering
\caption{Métricas de entrenamiento por época (VAE - MEL)}
\label{tab:vae_mel_train_epoch}
\begin{tabular}{rrrl}
\toprule
 epoch &  x\_mean &  x\_std & model\_name \\
\midrule
     0 & -0.2572 & 0.2618 &        VAE \\
     1 & -0.2572 & 0.2618 &        VAE \\
     2 & -0.2572 & 0.2618 &        VAE \\
     3 & -0.2572 & 0.2618 &        VAE \\
     4 & -0.2572 & 0.2618 &        VAE \\
     5 & -0.2572 & 0.2618 &        VAE \\
     6 & -0.2572 & 0.2618 &        VAE \\
     7 & -0.2572 & 0.2618 &        VAE \\
     8 & -0.2572 & 0.2618 &        VAE \\
     9 & -0.2572 & 0.2618 &        VAE \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Métricas de validación por época (VAE - MEL)}
\label{tab:vae_mel_val_epoch}
\begin{tabular}{rlrrrr}
\toprule
 epoch & model\_name &    val\_loss &  val\_recon\_loss &  val\_kl\_div &   beta \\
\midrule
     0 &        VAE & 917941.1875 &          0.2955 & 911838.0312 & 1.0067 \\
     0 &        VAE &  44198.5255 &          0.0315 &  43615.9243 & 1.0178 \\
     1 &        VAE &   8979.3878 &          0.0265 &   8572.7893 & 1.0474 \\
     2 &        VAE &   4912.4306 &          0.0294 &   4389.1960 & 1.1192 \\
     3 &        VAE &   3017.3802 &          0.0319 &   2377.8468 & 1.2689 \\
     4 &        VAE &   2368.9816 &          0.0298 &   1579.3012 & 1.5000 \\
     5 &        VAE &   1726.7845 &          0.0297 &    997.5138 & 1.7311 \\
     6 &        VAE &   1174.9416 &          0.0318 &    624.6872 & 1.8808 \\
     7 &        VAE &    963.5873 &          0.0306 &    493.4802 & 1.9526 \\
     8 &        VAE &    677.6251 &          0.0299 &    341.8721 & 1.9820 \\
     9 &        VAE &    455.1969 &          0.0301 &    228.3475 & 1.9933 \\
\bottomrule
\end{tabular}
\end{table}


\section{VAE con espectrogramas STFT}

\begin{table}[H]
\centering
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
=== Encoder ===
VAE_Encoder(
  (encoder): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
  )
  (genre_embedding): Embedding(5, 8)
  (conv_mu): Conv2d(264, 32, kernel_size=(1, 1), stride=(1, 1))
  (conv_logvar): Conv2d(264, 32, kernel_size=(1, 1), stride=(1, 1))
)

=== Decoder ===
VAE_Decoder(
  (genre_embedding): Embedding(5, 8)
  (lstm): LSTM(4136, 2048, num_layers=2, batch_first=True, dropout=0.3)
  (fc_out): Linear(in_features=2048, out_features=4128, bias=True)
  (deconv): Sequential(
    (0): ConvTranspose2d(32, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (1): LeakyReLU(negative_slope=0.2)
    (2): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (3): LeakyReLU(negative_slope=0.2)
    (4): ConvTranspose2d(128, 1, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
  )
)

\end{lstlisting}
\end{minipage}
\caption{Resumen de arquitectura del modelo VAE - STFT (encoder y decoder).}
\label{tab:vae_stft_model_summary}
\end{table}

\begin{table}
\centering
\caption{Métricas de entrenamiento por época (VAE - STFT)}
\label{tab:vae_stft_train_epoch}
\begin{tabular}{rrrl}
\toprule
 epoch &  x\_mean &  x\_std & model\_name \\
\midrule
     0 & -0.4675 & 0.2486 &        VAE \\
     1 & -0.4675 & 0.2486 &        VAE \\
     2 & -0.4675 & 0.2486 &        VAE \\
     3 & -0.4675 & 0.2486 &        VAE \\
     4 & -0.4675 & 0.2486 &        VAE \\
     5 & -0.4675 & 0.2486 &        VAE \\
     6 & -0.4675 & 0.2486 &        VAE \\
     7 & -0.4675 & 0.2486 &        VAE \\
     8 & -0.4675 & 0.2486 &        VAE \\
     9 & -0.4675 & 0.2486 &        VAE \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Métricas de validación por época (VAE - STFT)}
\label{tab:vae_stft_val_epoch}
\begin{tabular}{rlrrrr}
\toprule
 epoch & model\_name &     val\_loss &  val\_recon\_loss &   val\_kl\_div &   beta \\
\midrule
     0 &        VAE & 5038219.5000 &          0.4259 & 5004723.0000 & 1.0067 \\
     0 &        VAE &  120072.7835 &          0.0265 &  118500.9789 & 1.0179 \\
     1 &        VAE &   20753.7412 &          0.0270 &   19814.0174 & 1.0474 \\
     2 &        VAE &   10455.5080 &          0.0297 &    9341.8965 & 1.1192 \\
     3 &        VAE &    6531.9825 &          0.0262 &    5147.5633 & 1.2689 \\
     4 &        VAE &    4832.4354 &          0.0277 &    3221.6051 & 1.5000 \\
     5 &        VAE &    3528.9470 &          0.0262 &    2038.5912 & 1.7311 \\
     6 &        VAE &    2233.7787 &          0.0258 &    1187.6629 & 1.8808 \\
     7 &        VAE &    1474.2086 &          0.0255 &     754.9947 & 1.9526 \\
     8 &        VAE &    6087.2864 &          0.0343 &    3071.2460 & 1.9820 \\
     9 &        VAE &     674.2445 &          0.0275 &     338.2404 & 1.9933 \\
\bottomrule
\end{tabular}
\end{table}


\section{GAN con espectrogramas MEL}

\begin{table}[H]
\centering
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
=== Generator ===
GAN_Generator(
  (genre_embedding): Embedding(5, 8)
  (transformer): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(
            in_features=1032, out_features=1032, bias=True
          )
        )
        (linear1): Linear(in_features=1032, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=1032, bias=True)
        (norm1): LayerNorm((1032,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1032,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (fc_out): Linear(in_features=1032, out_features=1024, bias=True)
  (deconv): Sequential(
    (0): ConvTranspose2d(32, 256, kernel_size=(4, 1), stride=(2, 1), padding=(1, 0))
    (1): LeakyReLU(negative_slope=0.2)
    (2): ConvTranspose2d(256, 128, kernel_size=(4, 1), stride=(2, 1), padding=(1, 0))
    (3): LeakyReLU(negative_slope=0.2)
    (4): ConvTranspose2d(128, 1, kernel_size=(4, 1), stride=(2, 1), padding=(1, 0))
    (5): Tanh()
  )
)

=== Discriminator ===
GAN_Discriminator(
  (genre_embedding): Embedding(5, 8)
  (conv1): Conv2d(9, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(64, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(128, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
)

\end{lstlisting}
\end{minipage}
\caption{Resumen de arquitectura del modelo GAN - MEL (generador y discriminador).}
\label{tab:gan_mel_model_summary}
\end{table}

\begin{table}
\centering
\caption{Métricas de entrenamiento por época (GAN - MEL)}
\label{tab:gan_mel_train_epoch}
\begin{tabular}{rrlr}
\toprule
 epoch &  g\_loss & model\_name &  d\_loss \\
\midrule
     0 &  0.7552 &        GAN &  0.7320 \\
     1 &  0.7408 &        GAN &  0.7252 \\
     2 &  0.7187 &        GAN &  0.7450 \\
     3 &  0.7513 &        GAN &  0.7175 \\
     4 &  0.7477 &        GAN &  0.7185 \\
     5 &  0.7475 &        GAN &  0.7273 \\
     6 &  0.7554 &        GAN &  0.7267 \\
     7 &  0.7398 &        GAN &  0.7287 \\
     8 &  0.7386 &        GAN &  0.7245 \\
     9 &  0.7252 &        GAN &  0.7223 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Métricas de validación por época (GAN - MEL)}
\label{tab:gan_mel_val_epoch}
\begin{tabular}{rrlrr}
\toprule
 epoch &  model\_name &  val\_g\_loss &  val\_d\_loss \\
     0 &       GAN &      0.7349 &      0.7653 \\
     1 &       GAN &      0.7213 &      0.7399 \\
     2 &       GAN &      0.7696 &      0.7436 \\
     3 &       GAN &      0.7412 &      0.7686 \\
     4 &       GAN &      0.7618 &      0.7506 \\
     5 &       GAN &      0.7919 &      0.7191 \\
     6 &       GAN &      0.7815 &      0.7310 \\
     7 &       GAN &      0.7209 &      0.7609 \\
     8 &       GAN &      0.7804 &      0.7408 \\
     9 &       GAN &      0.8053 &      0.7414 \\
\bottomrule
\end{tabular}
\end{table}


\section{GAN con espectrogramas STFT}

\begin{table}[H]
\centering
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
=== Generator ===
GAN_Generator(
  (genre_embedding): Embedding(5, 8)
  (transformer): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(
            in_features=4104, out_features=4104, bias=True
          )
        )
        (linear1): Linear(in_features=4104, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=4104, bias=True)
        (norm1): LayerNorm((4104,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((4104,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (fc_out): Linear(in_features=4104, out_features=4096, bias=True)
  (deconv): Sequential(
    (0): ConvTranspose2d(32, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (1): LeakyReLU(negative_slope=0.2)
    (2): ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (3): LeakyReLU(negative_slope=0.2)
    (4): ConvTranspose2d(128, 1, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))
    (5): Tanh()
  )
)

=== Discriminator ===
GAN_Discriminator(
  (genre_embedding): Embedding(5, 8)
  (conv1): Conv2d(9, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(64, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(128, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
)

\end{lstlisting}
\end{minipage}
\caption{Resumen de arquitectura del modelo GAN - STFT (generador y discriminador).}
\label{tab:gan_stft_model_summary}
\end{table}

\begin{table}
\centering
\caption{Métricas de entrenamiento por época (GAN - STFT)}
\label{tab:gan_train_epoch}
\begin{tabular}{rrlr}
\toprule
 epoch &  g\_loss & model\_name &  d\_loss \\
\midrule
     0 &  0.8494 &        GAN &  0.7246 \\
     1 &  0.8537 &        GAN &  0.7188 \\
     2 &  0.7661 &        GAN &  0.7422 \\
     3 &  0.7758 &        GAN &  0.7260 \\
     4 &  0.7815 &        GAN &  0.7305 \\
     5 &  0.8113 &        GAN &  0.7195 \\
     6 &  0.7997 &        GAN &  0.7254 \\
     7 &  0.7752 &        GAN &  0.7359 \\
     8 &  0.7901 &        GAN &  0.7255 \\
     9 &  0.7866 &        GAN &  0.7191 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Métricas de validación por época (GAN - STFT)}
\label{tab:gan_val_epoch}
\begin{tabular}{rrlrr}
\toprule
 epoch &  model\_name &  val\_g\_loss &  val\_d\_loss \\
     0 &        GAN &      1.0907 &      1.0928 \\
     1 &        GAN &      1.2443 &      1.1420 \\
     2 &        GAN &      1.0848 &      1.1634 \\
     3 &        GAN &      1.0577 &      0.9072 \\
     4 &        GAN &      0.9103 &      0.9116 \\
     5 &        GAN &      0.9694 &      0.8109 \\
     6 &        GAN &      0.9015 &      0.8193 \\
     7 &        GAN &      0.8711 &      0.7959 \\
     8 &        GAN &      0.9561 &      0.7803 \\
     9 &        GAN &      0.8597 &      0.8345 \\
\bottomrule
\end{tabular}
\end{table}

